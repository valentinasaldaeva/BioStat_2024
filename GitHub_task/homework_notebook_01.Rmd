---
title: "automatization_notebook_04"
output:
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(tidyr)
library(ggbeeswarm)
library(RColorBrewer)
library(gridExtra)
library(rstatix)
```

# Чтение данных

В вашем варианте нужно использовать датасет healthcare-dataset-stroke-data.

```{r}
data <- read_csv("C:/Users/Алексей/Desktop/mag/statistics/biostatistics/healthcare-dataset-stroke-data.csv")
glimpse(data)
str(data)
```

# Выведите общее описание данных

```{r}
summary(data)

```

# Очистка данных

1) Уберите переменные, в которых пропущенных значений больше 20% или уберите субъектов со слишком большим количеством пропущенных значений. Или совместите оба варианта. Напишите обоснование, почему вы выбрали тот или иной вариант:

**Обоснование**: 

2) Переименуйте переменные в человекочитаемый вид (что делать с пробелами в названиях?);

3) В соответствии с описанием данных приведите переменные к нужному типу (numeric или factor);

4) Отсортируйте данные по возрасту по убыванию;

5) Сохраните в файл outliers.csv субъектов, которые являются выбросами (например, по правилу трёх сигм) — это необязательное задание со звёздочкой;

6) Присвойте получившийся датасет переменной "cleaned_data".

```{r}
#Переименуйте переменные в человекочитаемый вид - думаю, что названия переменных в этом датафрейме достаточно понятны и читаемы, поэтому не стала их переименовывать.
# количество пропущенных значений 
# Подсчет значений "Private" в колонке "work_type"
private_count <- data %>%
  filter(work_type == "Private") %>%
  nrow()
print(private_count)
# Подсчет значений "Unknown" в колонке "smoking_status"
unknown_count <- data %>% 
  filter(smoking_status == "Unknown") %>% 
  nrow()
print(unknown_count)
# Подсчет значений "N/A" в колонке "bmi"
NA_count <- data %>% 
  filter(bmi == "N/A") %>% 
  nrow()
print(NA_count)
# Подсчет значений "Other" в колонке "gender"
NA_count <- data %>% 
  filter(gender == "Other") %>% 
  nrow()
print(NA_count)
# Удаление слов "N/A", "Unknown" и "Private"
data_cleaned <- data %>%
  mutate_all(~replace(., . %in% c("N/A", "Unknown", "Private", "Other"), NA))

cleaned_data <- data_cleaned[, colSums(is.na(data_cleaned)) / nrow(data_cleaned) < 0.2] #удаляем переменные в которых пропущенных значений больше 20 %

# Замена значений в колонке "ever_married"
cleaned_data <- cleaned_data %>%
  mutate(ever_married = ifelse(ever_married == "No", 0, 
                              ifelse(ever_married == "Yes", 1, ever_married)))
#Замена значений в колонке "Residence_type"
# Замена значений в колонке "Residence_type"
cleaned_data <- cleaned_data %>%
  mutate(Residence_type = ifelse(Residence_type == "Rural", 0, 
                                ifelse(Residence_type == "Urban", 1, Residence_type)))

 # Замена значений в колонке "gender"
cleaned_data <- cleaned_data %>%
  mutate(gender = ifelse(gender == "Female", 0, 
                              ifelse(gender == "Male", 1, gender)))
# Сортировка данных в датафрейме cleaned_data по убыванию возраста
cleaned_data <- cleaned_data[order(-cleaned_data$age),]

```

# Сколько осталось переменных?

```{r}

# Количество переменных в cleaned_data
num_vars <- ncol(cleaned_data)
print(num_vars)

```

# Сколько осталось случаев?

```{r}

# Подсчет оставшихся случаев
remaining_cases <- nrow(cleaned_data)
print(remaining_cases)

```

# Есть ли в данных идентичные строки?

```{r}
# Подсчёт количества идентичных строк
duplicate_count <- sum(duplicated(cleaned_data))
cat("Количество идентичных строк:", duplicate_count)

```

# Сколько всего переменных с пропущенными значениями в данных и сколько пропущенных точек в каждой такой переменной?

```{r}
# Подсчет пропущенных значений в каждой переменной
missing_values <- sapply(cleaned_data, function(x) sum(is.na(x)))

# Фильтрация переменных с пропущенными значениями
variables_with_missing <- names(missing_values[missing_values > 0])

# Вывод результатов
cat("Количество переменных с пропущенными значениями:", length(variables_with_missing), "\n")
cat("Количество пропущенных значений в каждой переменной:\n")
print(missing_values[variables_with_missing])

```

# Описательные статистики

## Количественные переменные

1) Рассчитайте для всех количественных переменных для каждой группы (stroke):

1.1) Количество значений;

1.2) Количество пропущенных значений;

1.3) Среднее;

1.4) Медиану;

1.5) Стандартное отклонение;

1.6) 25% квантиль и 75% квантиль;

1.7) Интерквартильный размах;

1.8) Минимум;

1.9) Максимум;

1.10) 95% ДИ для среднего - задание со звёздочкой.

```{r}
cleaned_data <- cleaned_data %>%
  mutate(bmi = as.numeric(bmi))
# Расчеты для каждой группы
summary_stats_N <- cleaned_data %>%
  group_by(stroke) %>%
  summarize(
    # Количество значений
    n = n(),
    # Количество пропущенных значений
    n_missing = sum(is.na(age)), 
    n_missing_avg_glucose_level = sum(is.na(avg_glucose_level)),
    n_missing_bmi = sum(is.na(bmi)),
    # Среднее
    mean_age = mean(age, na.rm = TRUE),
    mean_avg_glucose_level = mean(avg_glucose_level, na.rm = TRUE), 
    mean_bmi = mean(bmi, na.rm = TRUE),
    # Медиана
    median_age = median(age, na.rm = TRUE),
    median_avg_glucose_level = median(avg_glucose_level, na.rm = TRUE),
    median_bmi = median(bmi, na.rm = TRUE),
    # 1.5) Стандартное отклонение
    sd_age = sd(age, na.rm = TRUE),
    sd_avg_glucose_level = sd(avg_glucose_level, na.rm = TRUE),
    sd_bmi = sd(bmi, na.rm = TRUE),
    # 1.6) 25% квантиль и 75% квантиль
    q25_age = quantile(age, 0.25, na.rm = TRUE),
    q75_age = quantile(age, 0.75, na.rm = TRUE),
    q25_avg_glucose_level = quantile(avg_glucose_level, 0.25, na.rm = TRUE),
    q75_avg_glucose_level = quantile(avg_glucose_level, 0.75, na.rm = TRUE),
    q25_bmi = quantile(bmi, 0.25, na.rm = TRUE),
    q75_bmi = quantile(bmi, 0.75, na.rm = TRUE),
    # 1.7) Интерквартильный размах
    iqr_age = q75_age - q25_age,
    iqr_avg_glucose_level = q75_avg_glucose_level - q25_avg_glucose_level,
    iqr_bmi = q75_bmi - q25_bmi,
    # 1.8) Минимум
    min_age = min(age, na.rm = TRUE),
    min_avg_glucose_level = min(avg_glucose_level, na.rm = TRUE),
    min_bmi = min(bmi, na.rm = TRUE),
    # 1.9) Максимум
    max_age = max(age, na.rm = TRUE),
    max_avg_glucose_level = max(avg_glucose_level, na.rm = TRUE),
    max_bmi = max(bmi, na.rm = TRUE),
    # 1.10) 95% ДИ для среднего
    ci_mean_age = 1.96 * sd_age / sqrt(n),
    ci_mean_avg_glucose_level = 1.96 * sd_avg_glucose_level / sqrt(n),
    ci_mean_bmi = 1.96 * sd_bmi / sqrt(n)
  )

# Вывод результатов
print(summary_stats_N)

# Транспонирование таблицы
transposed_summary_stats_N <- summary_stats_N %>%
  pivot_longer(cols = -stroke, names_to = "metric", values_to = "value") %>%
  pivot_wider(names_from = stroke, values_from = value)

# Вывод результатов
print(transposed_summary_stats_N)

```

## Категориальные переменные

1) Рассчитайте для всех категориальных переменных для каждой группы (stroke):

1.1) Абсолютное количество;

1.2) Относительное количество внутри группы;

1.3) 95% ДИ для доли внутри группы - задание со звёздочкой.

```{r}

# Преобразование типа данных
cleaned_data <- cleaned_data %>%
  mutate(
    hypertension = as.numeric(hypertension),
    heart_disease = as.numeric(heart_disease),
    ever_married = as.numeric(ever_married),
    Residence_type = as.numeric(Residence_type)
  )

# Расчеты для каждой группы
summary_stats_K <- cleaned_data %>%
  group_by(stroke) %>%
  summarize(
    # Абсолютное количество
    n_hypertension = sum(hypertension),
    n_heart_disease = sum(heart_disease),
    n_ever_married = sum(ever_married),
    n_Residence_type = sum(Residence_type),
    
    # Относительное количество внутри группы
    prop_hypertension = n_hypertension / n(),
    prop_heart_disease = n_heart_disease / n(),
    prop_ever_married = n_ever_married / n(),
    prop_Residence_type = n_Residence_type / n()
  )
# Вывод результатов
print(summary_stats_K)
# Транспонирование таблицы

transposed_summary_stats_K <- summary_stats_K %>%
  pivot_longer(cols = -stroke, names_to = "metric", values_to = "value") %>%
  pivot_wider(names_from = stroke, values_from = value)

# Вывод результатов
print(transposed_summary_stats_K)


```

# Визуализация

## Количественные переменные

1) Для каждой количественной переменной сделайте боксплоты по группам. Расположите их либо на отдельных рисунках, либо на одном, но читаемо;

2) Наложите на боксплоты beeplots - задание со звёздочкой.

3) Раскрасьте боксплоты с помощью библиотеки RColorBrewer.

```{r}
# 1) Боксплоты по группам (0 и 1)
# Создаю боксплоты для каждой количественной переменной
age_plot <- ggplot(cleaned_data, aes(x = factor(stroke), y = age, fill = factor(stroke))) +
  geom_boxplot() +
  labs(x = "Stroke (0 - No, 1 - Yes)", y = "Age")

glucose_plot <- ggplot(cleaned_data, aes(x = factor(stroke), y = avg_glucose_level, fill = factor(stroke))) +
  geom_boxplot() +
  labs(x = "Stroke (0 - No, 1 - Yes)", y = "Average Glucose Level")

bmi_plot <- ggplot(cleaned_data, aes(x = factor(stroke), y = bmi, fill = factor(stroke))) +
  geom_boxplot() +
  labs(x = "Stroke (0 - No, 1 - Yes)", y = "BMI")

# 2) Раскрасьте боксплоты с помощью библиотеки RColorBrewer
# Выбираю цветовую палитру
palette <- brewer.pal(length(unique(cleaned_data$cardio)), "Paired")

age_plot <- age_plot + scale_fill_manual(values = palette)
glucose_plot <- glucose_plot + scale_fill_manual(values = palette)
bmi_plot <- bmi_plot + scale_fill_manual(values = palette)

# Боксплоты
grid.arrange(age_plot, glucose_plot, bmi_plot, ncol = 2)

```

## Категориальные переменные

1) Сделайте подходящие визуализации категориальных переменных. Обоснуйте, почему выбрали именно этот тип.

```{r}
# Категориальные переменные
categorical_vars <- c("gender", "hypertension", "heart_disease", "ever_married", "Residence_type", "stroke")

# Создаем визуализации для каждой категориальной переменной
for (var in categorical_vars) {
  # Таблица частот
  freq_table <- table(cleaned_data[[var]])
  
  # Визуализация
  if (length(unique(cleaned_data[[var]])) <= 6) {
    # Если количество уникальных значений <= 6, использую барплот
    p <- ggplot(data.frame(variable = names(freq_table), 
                          count = as.numeric(freq_table)), 
                aes(x = variable, y = count, fill = variable)) +
      geom_bar(stat = "identity") +
      labs(x = var, y = "Count") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  } else {
    # Если количество уникальных значений > 6, использую диаграмму кольца (pie chart)
    p <- ggplot(data.frame(variable = names(freq_table), 
                          count = as.numeric(freq_table)), 
                aes(x = "", y = count, fill = variable)) +
      geom_bar(stat = "identity", width = 1) +
      coord_polar("y", start = 0) +
      labs(x = NULL, y = NULL, fill = var) +
      theme_void()
  }
  
  # Раскрашиваю визуализацию с помощью RColorBrewer
  palette <- brewer.pal(length(unique(cleaned_data[[var]])), "Paired")
  p <- p + scale_fill_manual(values = palette)
  print(p)
}


```


# Статистические оценки

## Проверка на нормальность

1) Оцените каждую переменную на соответствие нормальному распределению с помощью теста Шапиро-Уилка. Какие из переменных являются нормальными и как как вы это поняли?

```{r}
#set.seed(123)
sample_data <- cleaned_data[sample(nrow(cleaned_data), 5000), ]

# Количественные переменные
quant_vars <- c("age", "avg_glucose_level", "bmi")

# Проверяем нормальность распределения
for (var in quant_vars) {
  # Тест Шапиро-Уилка
  shapiro_test <- shapiro.test(sample_data[[var]])
  
  # Выводим результат
  print(shapiro_test)
}
#Если p-value > 0.05, то мы можем считать, что распределение переменной соответствует нормальному. Иначе, если p-value < 0.05, то распределение переменной не является нормальным.

#В нашем случае:
#- age имеет p-value = 2.2e-16 < 0.05, поэтому она не соответствует нормальному распределению.
#- avg_glucose_level имеет p-value < 2.2e-16 < 0.05, поэтому она не соответствует нормальному распределению.
#- bmi имеет p-value = 2.2e-16 -15 < 0.05, поэтому она также не соответствует нормальному распределению.

#Таким образом, ни одна из количественных переменных (age, avg_glucose_level, bmi) не соответствует нормальному распределению.

#Для категориальных переменных (gender, hypertension, heart_disease, ever_married, Residence_type, stroke) тест Шапиро-Уилка не применим, так как он предназначен только для количественных переменных. Для категориальных переменных можно использовать другие тесты, например, chi-square или Fisher's exact test.

```

2) Постройте для каждой количественной переменной QQ-плот. Отличаются ли выводы от теста Шапиро-Уилка? Какой метод вы бы предпочли и почему?

```{r}


# Строим QQ-плоты
par(mfrow = c(1, 3))
for (var in quant_vars) {
  qqnorm(cleaned_data[[var]], main = paste("QQ-plot for", var))
  qqline(cleaned_data[[var]])
}
#1. Для переменной age точки на графике отклоняются от прямой линии, указывая на то, что распределение этой переменной не является нормальным.
#2. Для переменной avg_glucose_level точки также сильно отклоняются от прямой линии, подтверждая вывод о ненормальном распределении.
#3. Для переменной bmi точки на графике в целом следуют прямой линии, но все же есть некоторое отклонение, особенно в хвостах распределения.

#Сравнивая эти результаты с выводами теста Шапиро-Уилка, видим, что они полностью совпадают. Тест Шапиро-Уилка также показал, что ни одна из количественных переменных не соответствует нормальному распределению.

#Какой метод вы бы предпочли и почему? Я бы предпочла метод Шапиро-Уилка т.к. получила точные цифры, на которые могу ориентироваться. Метот построения графика QQ-плот можно использовать если необходимо оценить всю выборку (которая имеет более 5000 значений).


```

3) Ниже напишите, какие ещё методы проверки на нормальность вы знаете и какие у них есть ограничения.

Гистограмма: Позволяет оценить симметрию и форму распределения, но может быть сильно зависима от выбора количества интервалов.
Тест Колмогорова-Смирнова: Основан на максимальном отклонении эмпирической функции распределения от теоретической. Чувствителен к любым отклонениям от нормальности, но может быть менее мощным тестом.


## Сравнение групп

1) Сравните группы (переменная **stroke**) по каждой переменной (как количественной, так и категориальной). Для каждой переменной выберите нужный критерий и кратко обоснуйте его выбор в комментариях.

```{r}
# Сравнение количественных переменных
for (var in c("age", "avg_glucose_level", "bmi")) {

    # Распределение не является нормальным, используем критерий Манна-Уитни
    print(paste0("Mann-Whitney test for ", var, ":"))
    print(wilcox.test(cleaned_data[[var]] ~ cleaned_data$stroke))
  }


# Сравнение категориальных переменных
for (var in c("gender", "hypertension", "heart_disease", "ever_married", "Residence_type")) {
  print(paste0("Chi-square test for ", var, ":"))
  print(chisq.test(cleaned_data[[var]], cleaned_data$stroke))
}


```

# Далее идут **необязательные** дополнительные задания, которые могут принести вам дополнительные баллы в том числе в случае ошибок в предыдущих

## Корреляционный анализ

1) Создайте корреляционную матрицу с визуализацией и поправкой на множественные сравнения. Объясните, когда лучше использовать корреляционные матрицы и в чём минусы и плюсы корреляционных исследований.

```{r}
library(corrplot)
library(Hmisc)
# Корреляционная матрица
cor_matrix <- cor(cleaned_data[, c("age", "avg_glucose_level", "bmi")], use = "complete.obs")

# Визуализация корреляционной матрицы
corrplot(cor_matrix, method = "color", 
         type = "upper", 
         order = "hclust",
         tl.col = "black", 
         tl.srt = 45,
         addCoef.col = "black",
         number.cex = 0.7,
         p.mat = cor.mtest(cleaned_data[, c("age", "avg_glucose_level", "bmi")], 
                          method = "pearson", 
                          exact = FALSE)$p,
         sig.level = 0.05,
         insig = "blank")

#Объяснение корреляционных матриц:

#Корреляционные матрицы - это инструмент для визуализации и анализа взаимосвязей между количественными переменными. Они показывают коэффициенты корреляции, которые варьируются от -1 до 1 и отражают направление (положительное или отрицательное) и силу связи между парами переменных.

#Когда лучше использовать корреляционные матрицы?

#- Корреляционные матрицы используются на ранних этапах исследования, когда необходимо понять, какие переменные могут быть связаны друг с другом.
#- Корреляционные матрицы могут быть использованы для формирования гипотез о причинно-следственных связях между переменными, которые затем можно проверить с помощью более сложных методов анализа.

#Плюсы и минусы корреляционных исследований:

#Плюсы:
#- Простота интерпретации и визуализации взаимосвязей между переменными.
#- Выявление потенциальных предикторов зависимых переменных.
#- Предварительный анализ данных перед применением более сложных методов.

#Минусы:
#- Корреляция не означает причинно-следственную связь. Она показывает только, что две переменные связаны, но не объясняет, почему.
#- Корреляции могут быть ложными, особенно при большом количестве сравнений (проблема множественных сравнений).
#- Сила корреляции не означает практической значимости. Даже сильная корреляция может иметь небольшое практическое значение.
#- Корреляционные исследования не позволяют делать выводы о направлении причинно-следственных связей.

#В нашем учла проблему множественных сравнений, применив поправку p-значений с помощью функции cor.mtest(). Это позволяет более достоверно интерпретировать результаты корреляционного анализа.

```

## Моделирование

1) Постройте регрессионную модель для переменной **stroke**. Опишите процесс построения

```{r}
# типы переменных
str(cleaned_data)

# регрессионная модель
model <- glm(stroke ~ age + avg_glucose_level + bmi + gender + hypertension + heart_disease + ever_married + Residence_type, 
             data = cleaned_data, 
             family = binomial)

# Вывод результатов модели
summary(model)

#Процесс построения модели:

#1. Проверка типов переменных: Сначала проверила типы переменных в датафрейме, чтобы убедиться, что они соответствуют требованиям для построения регрессионной модели. Переменная "stroke" является бинарной (0 или 1), а остальные переменные - количественными и категориальными.

#2. Выбор модели: Поскольку зависимая переменная "stroke" является бинарной, использовала обобщенную линейную модель (GLM) с семейством "binomial", которая позволяет моделировать вероятность наступления события (в данном случае, наличие инсульта).

#3. Спецификация модели: В формуле модели включила все интересующие предикторы: количественные переменные "age", "avg_glucose_level", "bmi" и категориальные переменные "gender", "hypertension", "heart_disease", "ever_married" и "Residence_type".

#4. Оценка модели: Функция glm() оценивает параметры модели и возвращает объект модели, который сохранила в переменную model. Далее использую функцию summary() для вывода результатов модели.

#Регрессионную модель для переменной **stroke** может быть использована для прогнозирования вероятности наступления события "stroke" на основе значений предикторов. Кроме того, результаты модели помогут выявить наиболее важные факторы, связанные с риском инсульта.

```




